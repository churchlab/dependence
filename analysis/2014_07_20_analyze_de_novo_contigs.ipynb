{
 "metadata": {
  "name": "",
  "signature": "sha256:9c22e9ee3805d8ed61986c0f11c3ec0e883e6795adf6b34d6ef5a77614e7edb2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import random\n",
      "import re\n",
      "import time\n",
      "\n",
      "from Bio import SeqIO\n",
      "from Bio.Blast import NCBIWWW\n",
      "from Bio.Blast import NCBIXML\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "VELVET_DATA_DIR = os.path.join('data/velvet_assemblies_mobile_lon_clpX/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Create Master Spreadsheet\n",
      "\n",
      "Let's make a spreadsheet out of the assemblies so that we can more easily triage/compare."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def parse_contig_info(name):\n",
      "    \"\"\"Parses contig info from the name generated by velvet.\n",
      "    \"\"\"\n",
      "    m = re.match(r'(?P<id>.*)_length_(?P<length>[\\d]+)_cov_(?P<coverage>[\\d\\.]+)', name)\n",
      "    return {\n",
      "        'id': m.group('id'),\n",
      "        'length': int(m.group('length')),\n",
      "        'coverage': float(m.group('coverage'))\n",
      "    }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_dict_list = []\n",
      "CONTIG_SOURCE_FASTAS = os.path.join(VELVET_DATA_DIR, 'fasta')\n",
      "for filename in os.listdir(CONTIG_SOURCE_FASTAS):\n",
      "    parts = os.path.splitext(filename)\n",
      "    if len(parts) < 2 or parts[1] != '.fa':\n",
      "        continue\n",
      "    sample = parts[0]\n",
      "    full_path = os.path.join(CONTIG_SOURCE_FASTAS, filename)\n",
      "    with open(full_path) as fh:\n",
      "        records = SeqIO.parse(fh, 'fasta')\n",
      "        for r in records:\n",
      "            data_dict = {\n",
      "                'sample': sample,\n",
      "                'sequence': str(r.seq),\n",
      "                'note': '',\n",
      "                'flag': '',\n",
      "            }\n",
      "            data_dict.update(parse_contig_info(r.name))\n",
      "            data_dict['id'] = sample + '_' + data_dict['id']\n",
      "            data_dict_list.append(data_dict)\n",
      "contig_data_df = pd.DataFrame(data_dict_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sort the columns and write output to master csv."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sort_dataframe_columns(df, priority_columns):\n",
      "    \"\"\"Returns new DataFrame with priority columns first, then remaining columns.\n",
      "    \"\"\"\n",
      "    columns = list(df.columns)\n",
      "    reordered_columns = []\n",
      "    for col in priority_columns:\n",
      "        assert col in columns, \"Missing %s\" % col\n",
      "        columns.remove(col)\n",
      "        reordered_columns.append(col)\n",
      "    reordered_columns.extend(columns)\n",
      "    return df[reordered_columns]\n",
      "contig_data_df = sort_dataframe_columns(contig_data_df, ['id', 'sample', 'length', 'coverage', 'flag', 'note', 'sequence'])\n",
      "contig_data_df.sort(columns=['length', 'sample'], ascending=False).to_csv(\n",
      "        os.path.join(VELVET_DATA_DIR, 'contig_data_all_8_samples.csv'),\n",
      "        index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Filter to contigs that include lon region"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "START_OF_LON_SEQ = 'ATGAATCCTGAGCGTTCTGAACGCATTGAAATCCCCGTATTGCCG'\n",
      "contigs_with_lon_df = contig_data_df[contig_data_df.sequence.apply(lambda seq: START_OF_LON_SEQ in seq)]\n",
      "contigs_with_lon_df.sort(columns=['length', 'sample'], ascending=False).to_csv(\n",
      "        os.path.join(VELVET_DATA_DIR, 'contig_data_all_8_samples_lon_only.csv'),\n",
      "        index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "PARTIAL_CLPX_SEQ = 'GGTCGCGGTATACAACCATTACAAACGTCTG'\n",
      "#PARTIAL_CLPX_SEQ = 'GCGCCTGAGAATGGCATTTGCGTCGTCG'\n",
      "contigs_with_clpX_df = contig_data_df[contig_data_df.sequence.apply(lambda seq: PARTIAL_CLPX_SEQ in seq)]\n",
      "contigs_with_clpX_df.sort(columns=['length', 'sample'], ascending=False).to_csv(\n",
      "        os.path.join(VELVET_DATA_DIR, 'contig_data_all_8_samples_clpX_only.csv'),\n",
      "        index=False)\n",
      "print contigs_with_clpX_df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "                        id        sample  length    coverage flag note  \\\n",
        "42    tyrS.d8_Esc1_NODE_71  tyrS.d8_Esc1    4180   60.526318             \n",
        "192    tyrS.d8_DEP_NODE_78   tyrS.d8_DEP    4176   82.891762             \n",
        "326    adk.d6_Esc1_NODE_55   adk.d6_Esc1     716   48.255585             \n",
        "480  tyrS.d8_Esc3_NODE_162  tyrS.d8_Esc3    4200  160.299042             \n",
        "624    adk.d6_Esc2_NODE_70   adk.d6_Esc2    1508  103.275864             \n",
        "766     adk.d6_DEP_NODE_91    adk.d6_DEP    4210  156.704514             \n",
        "908   tyrS.d8_Esc2_NODE_63  tyrS.d8_Esc2    1925   76.788574             \n",
        "\n",
        "                                              sequence  \n",
        "42   GCGCAACTGTGCCGCTATACTTATCCAGGGCGGCACAACGCTGTAA...  \n",
        "192  TGCCAGAGGCGCAACTGTGCCGCTATACTTATCCAGGGCGGCACAA...  \n",
        "326  CCTGAGAATGGCATTTGCGTCGTCGTGTGCGGCACAAAGAACAAAG...  \n",
        "480  TCTGACCCATCGTAATTGATGCCAGAGGCGCAACTGTGCCGCTATA...  \n",
        "624  CCCATCGTAATTGATGCCAGAGGCGCAACTGTGCCGCTATACTTAT...  \n",
        "766  TGACCCATCGTAATTGATGCCAGAGGCGCAACTGTGCCGCTATACT...  \n",
        "908  CTTATCCAGGGCGGCACAACGCTGTAAGCGGCTTGCGCCTGAGAAT...  \n"
       ]
      }
     ],
     "prompt_number": 47
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "MIDDLE_INSL1_SEQ = 'CAGCGCTGAATGGCGACTACATATGGGATATGATCCTCATAC'\n",
      "contigs_with_insl1 = contig_data_df[contig_data_df.sequence.apply(lambda seq: MIDDLE_INSL1_SEQ in seq)]\n",
      "contigs_with_insl1.sort(columns=['length', 'sample'], ascending=False).to_csv(\n",
      "        os.path.join(VELVET_DATA_DIR, 'contig_data_all_8_samples_insl1.csv'),\n",
      "        index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## BLAST all the sequences using NCBI."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we kick off a bunch of requests in parallel (we modify Biopython NCBIWWW qblast function to just return the `rid` of the request).\n",
      "\n",
      "Then, we'll collect our results as they become ready."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### First step: Put jobs on queue"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "assert False, \"Prevent from running unintentionally.\"\n",
      "num_rows = len(contig_data_df)\n",
      "for idx, row in contig_data_df.iterrows():\n",
      "    contig_id = row['id']\n",
      "    if contig_id in contig_id_to_ncbi_rid:\n",
      "        continue\n",
      "    print 'Running %d of %d' % (idx + 1, num_rows)\n",
      "    rid = NCBIWWW.qblast('blastn', 'nr', row['sequence'])\n",
      "    contig_id_to_ncbi_rid[contig_id] = rid\n",
      "    time.sleep(random.randint(3, 7))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "AssertionError",
       "evalue": "Prevent from running unintentionally.",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-27-2117d8af0d1d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32massert\u001b[0m \u001b[0mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"Prevent from running unintentionally.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mnum_rows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontig_data_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontig_data_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mcontig_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'id'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontig_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcontig_id_to_ncbi_rid\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mAssertionError\u001b[0m: Prevent from running unintentionally."
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Save the map from contig id to NCBI job id."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CONTIG_ID_TO_NCBI_MAP_CSV = os.path.join(VELVET_DATA_DIR, 'contig_id_to_ncbi_rid_map.csv')\n",
      "with open(CONTIG_ID_TO_NCBI_MAP_CSV, 'w') as fh:\n",
      "    for contig_id, rid in contig_id_to_ncbi_rid.iteritems():\n",
      "        fh.write(contig_id + ',' + rid + '\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we'll request results from NCBI and write to an output file. We'll grab both xml and html just in case. We write this function so that it can be run repeatedly in the case that jobs are still processing, but avoids making a repeated request for a job that we've already downloaded the data from."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reload(NCBIWWW)\n",
      "BLAST_XML_OUTPUT_DIR = os.path.join(VELVET_DATA_DIR, 'blast_xml')\n",
      "BLAST_HTML_OUTPUT_DIR = os.path.join(VELVET_DATA_DIR, 'blast_html')\n",
      "def write_results_to_file(contig_id, rid, xml_or_html):\n",
      "    \"\"\"Performs BLAST and saves to xml.\n",
      "    \"\"\"\n",
      "    assert xml_or_html in ['XML', 'HTML']\n",
      "    \n",
      "    type_to_ext = {'XML': '.xml', 'HTML': '.html'}\n",
      "    dest_ext = type_to_ext[xml_or_html]\n",
      "    type_to_output_dir = {'XML': BLAST_XML_OUTPUT_DIR, 'HTML': BLAST_HTML_OUTPUT_DIR}\n",
      "    output_dir = type_to_output_dir[xml_or_html]\n",
      "    dest = os.path.join(output_dir, contig_id + dest_ext)\n",
      "    \n",
      "    # Don't make request if data already exists.\n",
      "    if os.path.exists(dest):\n",
      "        print 'already done ...'\n",
      "        return False\n",
      "   \n",
      "    # Prepare and make request.\n",
      "    message = 'ALIGNMENTS=500&DESCRIPTIONS=500&FORMAT_TYPE={format_type}&RID={rid}&CMD=Get'.format(\n",
      "        format_type=xml_or_html,\n",
      "        rid=rid,\n",
      "    )\n",
      "    request = _Request(\"http://blast.ncbi.nlm.nih.gov/Blast.cgi\",\n",
      "            message, {\"User-Agent\":\"BiopythonClient\"})\n",
      "    print 'making request ...'\n",
      "    handle = _urlopen(request)\n",
      "    time.sleep(random.randint(2, 3))\n",
      "    results = _as_string(handle.read())\n",
      "    \n",
      "    # Determine whether ready.\n",
      "    is_ready = False\n",
      "    if \"Status=\" not in results:\n",
      "        is_ready = True\n",
      "    else:\n",
      "        i = results.index(\"Status=\")\n",
      "        j = results.index(\"\\n\", i)\n",
      "        status = results[i+len(\"Status=\"):j].strip()\n",
      "        if status.upper() == \"READY\":\n",
      "            is_ready = True\n",
      "        \n",
      "    # Write if rady.\n",
      "    if is_ready:\n",
      "        print 'ready, writing results ...'\n",
      "        with open(dest, 'w') as output_fh:\n",
      "            output_fh.write(results)\n",
      "    else:\n",
      "        print 'still waiting ...'\n",
      "    \n",
      "    return True\n",
      "\n",
      "count = 1\n",
      "total = len(contig_id_to_ncbi_rid)\n",
      "for contig_id, rid in contig_id_to_ncbi_rid.iteritems():\n",
      "    print '>>>>>>>>>>>>Running %d of %d' % (count, total)\n",
      "    write_results_to_file(contig_id, rid, 'XML')\n",
      "    write_results_to_file(contig_id, rid, 'HTML')\n",
      "    count += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Fix HTML Results\n",
      "\n",
      "**UPDATE:** It turns out that it's not enough to update static links to fix html files as the links to specific alignments go to a cached page (which expire after a couple days). Saving this code for posterity anyway.\n",
      "\n",
      "The HTML files have broken static links which prevent them from displaying correctly. The static links are relative rather than absolute. These can be fixed with regular expression searches."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fix_static_html():\n",
      "    for filename in os.listdir(BLAST_XML_OUTPUT_DIR):\n",
      "        if not os.path.splitext(filename)[1] == '.html':\n",
      "            continue\n",
      "        full_path = os.path.join(BLAST_XML_OUTPUT_DIR, filename)\n",
      "        new_html_path = os.path.splitext(full_path)[0] + '.mod.html'\n",
      "        with open(new_html_path, 'w') as output_fh:\n",
      "            with open(full_path) as input_fh:\n",
      "                for line in input_fh:\n",
      "                    mod_line = line\n",
      "                    if re.search(r'link rel=\"stylesheet\"', line):\n",
      "                        mod_line = line.replace('href=\"', 'href=\"http://blast.ncbi.nlm.nih.gov/')\n",
      "                    elif re.search(r'<script.*javascript.*src=\"', line):\n",
      "                        mod_line = line.replace('src=\"', 'src=\"http://blast.ncbi.nlm.nih.gov/')          \n",
      "                    elif re.search(r'img.*src=', line):\n",
      "                        mod_line = line.replace('src=\"', 'src=\"http://blast.ncbi.nlm.nih.gov/')\n",
      "                    else:\n",
      "                        mod_line = line\n",
      "                    output_fh.write(mod_line)\n",
      "# fix_static_html()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Parse xml data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "contig_id_to_ncbi_alignment_data_map = {}\n",
      "for filename in os.listdir(BLAST_XML_OUTPUT_DIR):\n",
      "    full_path = os.path.join(BLAST_XML_OUTPUT_DIR, filename)\n",
      "    contig_id = os.path.splitext(filename)[0]\n",
      "    assert not contig_id in contig_id_to_ncbi_alignment_data_map\n",
      "    with open(full_path) as fh:\n",
      "        parsed = NCBIXML.parse(fh)\n",
      "        x = parsed.next()\n",
      "        # We want to grab the highest alignment, unless one of them is MG1655\n",
      "        # which will make it easier to query.\n",
      "        a = x.alignments[0]\n",
      "        for other_a in x.alignments[1:30]:\n",
      "            if 'U00096.3' in other_a.hit_id:\n",
      "                a = other_a\n",
      "                break\n",
      "        h = a.hsps[0]\n",
      "        data_map = {\n",
      "            'aln_hit_id': a.hit_id,\n",
      "            'aln_hit_def': a.hit_def,\n",
      "            'aln_query_start': h.query_start,\n",
      "            'aln_query_end': h.query_end,\n",
      "            'aln_sbjct_start': h.sbjct_start,\n",
      "            'aln_sbjct_end': h.sbjct_end\n",
      "        }\n",
      "        contig_id_to_ncbi_alignment_data_map[contig_id] = data_map"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "contig_id_to_ncbi_alignment_data_map\n",
      "ncbi_alignment_data = []\n",
      "for contig_id, data in contig_id_to_ncbi_alignment_data_map.iteritems():\n",
      "    data['id'] = contig_id\n",
      "    ncbi_alignment_data.append(data)\n",
      "blast_data_df = pd.DataFrame(ncbi_alignment_data)\n",
      "blast_data_df['aln_size'] = abs(blast_data_df.aln_sbjct_end - blast_data_df.aln_sbjct_start)\n",
      "blast_data_df = sort_dataframe_columns(blast_data_df,\n",
      "        ['id', 'aln_hit_id', 'aln_hit_def', 'aln_size', 'aln_query_start', 'aln_query_end', 'aln_sbjct_start', 'aln_sbjct_end'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Combine with previous DataFrame."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "contig_data_with_blast_df = pd.merge(contig_data_df, blast_data_df, how='inner', on='id')\n",
      "contig_data_with_blast_df.sort(columns=['length', 'sample'], ascending=False).to_csv(\n",
      "        os.path.join(VELVET_DATA_DIR, 'contig_data_all_8_samples.csv'),\n",
      "        index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Scratch"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}