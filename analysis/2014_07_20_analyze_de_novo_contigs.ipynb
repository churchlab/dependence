{
 "metadata": {
  "name": "",
  "signature": "sha256:d72ec5d99e2d832ff7d7658efafdcabe52be8135277b9ac183384b396822c433"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import os\n",
      "import random\n",
      "import re\n",
      "import time\n",
      "\n",
      "from Bio import SeqIO\n",
      "from Bio.Blast import NCBIWWW\n",
      "from Bio.Blast import NCBIXML\n",
      "import pandas as pd"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 271
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "VELVET_DATA_DIR = os.path.join('data/velvet_assemblies/')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Create Master Spreadsheet\n",
      "\n",
      "Let's make a spreadsheet out of the assemblies so that we can more easily triage/compare."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "def parse_contig_info(name):\n",
      "    \"\"\"Parses contig info from the name generated by velvet.\n",
      "    \"\"\"\n",
      "    m = re.match(r'(?P<id>.*)_length_(?P<length>[\\d]+)_cov_(?P<coverage>[\\d\\.]+)', name)\n",
      "    return {\n",
      "        'id': m.group('id'),\n",
      "        'length': int(m.group('length')),\n",
      "        'coverage': float(m.group('coverage'))\n",
      "    }"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "data_dict_list = []\n",
      "CONTIG_SOURCE_FASTAS = os.path.join(VELVET_DATA_DIR, 'fasta')\n",
      "for filename in os.listdir(CONTIG_SOURCE_FASTAS):\n",
      "    parts = os.path.splitext(filename)\n",
      "    if len(parts) < 2 or parts[1] != '.fa':\n",
      "        continue\n",
      "    sample = parts[0]\n",
      "    full_path = os.path.join(CONTIG_SOURCE_FASTAS, filename)\n",
      "    with open(full_path) as fh:\n",
      "        records = SeqIO.parse(fh, 'fasta')\n",
      "        for r in records:\n",
      "            data_dict = {\n",
      "                'sample': sample,\n",
      "                'sequence': str(r.seq),\n",
      "                'note': '',\n",
      "                'flag': '',\n",
      "            }\n",
      "            data_dict.update(parse_contig_info(r.name))\n",
      "            data_dict['id'] = sample + '_' + data_dict['id']\n",
      "            data_dict_list.append(data_dict)\n",
      "contig_data_df = pd.DataFrame(data_dict_list)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 266
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Sort the columns and write output to master csv."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def sort_dataframe_columns(df, priority_columns):\n",
      "    \"\"\"Returns new DataFrame with priority columns first, then remaining columns.\n",
      "    \"\"\"\n",
      "    columns = list(df.columns)\n",
      "    reordered_columns = []\n",
      "    for col in priority_columns:\n",
      "        assert col in columns, \"Missing %s\" % col\n",
      "        columns.remove(col)\n",
      "        reordered_columns.append(col)\n",
      "    reordered_columns.extend(columns)\n",
      "    return df[reordered_columns]\n",
      "contig_data_df = sort_dataframe_columns(contig_data_df, ['id', 'sample', 'length', 'coverage', 'flag', 'note', 'sequence'])\n",
      "contig_data_df.sort(columns=['length', 'sample'], ascending=False).to_csv(\n",
      "        os.path.join(VELVET_DATA_DIR, 'contig_data_all_8_samples.csv'),\n",
      "        index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 313
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## BLAST all the sequences using NCBI."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we kick off a bunch of requests in parallel (we modify Biopython NCBIWWW qblast function to just return the `rid` of the request).\n",
      "\n",
      "Then, we'll collect our results as they become ready."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### First step: Put jobs on queue"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "num_rows = len(contig_data_df)\n",
      "for idx, row in contig_data_df.iterrows():\n",
      "    contig_id = row['id']\n",
      "    if contig_id in contig_id_to_ncbi_rid:\n",
      "        continue\n",
      "    print 'Running %d of %d' % (idx + 1, num_rows)\n",
      "    rid = NCBIWWW.qblast('blastn', 'nr', row['sequence'])\n",
      "    contig_id_to_ncbi_rid[contig_id] = rid\n",
      "    time.sleep(random.randint(3, 7))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 264
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Save the map from contig id to NCBI job id."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "CONTIG_ID_TO_NCBI_MAP_CSV = os.path.join(VELVET_DATA_DIR, 'contig_id_to_ncbi_rid_map.csv')\n",
      "with open(CONTIG_ID_TO_NCBI_MAP_CSV, 'w') as fh:\n",
      "    for contig_id, rid in contig_id_to_ncbi_rid.iteritems():\n",
      "        fh.write(contig_id + ',' + rid + '\\n')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 265
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next we'll request results from NCBI and write to an output file. We'll grab both xml and html just in case. We write this function so that it can be run repeatedly in the case that jobs are still processing, but avoids making a repeated request for a job that we've already downloaded the data from."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "reload(NCBIWWW)\n",
      "BLAST_XML_OUTPUT_DIR = os.path.join(VELVET_DATA_DIR, 'blast_xml')\n",
      "BLAST_HTML_OUTPUT_DIR = os.path.join(VELVET_DATA_DIR, 'blast_html')\n",
      "def write_results_to_file(contig_id, rid, xml_or_html):\n",
      "    \"\"\"Performs BLAST and saves to xml.\n",
      "    \"\"\"\n",
      "    assert xml_or_html in ['XML', 'HTML']\n",
      "    \n",
      "    type_to_ext = {'XML': '.xml', 'HTML': '.html'}\n",
      "    dest_ext = type_to_ext[xml_or_html]\n",
      "    type_to_output_dir = {'XML': BLAST_XML_OUTPUT_DIR, 'HTML': BLAST_HTML_OUTPUT_DIR}\n",
      "    output_dir = type_to_output_dir[xml_or_html]\n",
      "    dest = os.path.join(output_dir, contig_id + dest_ext)\n",
      "    \n",
      "    # Don't make request if data already exists.\n",
      "    if os.path.exists(dest):\n",
      "        print 'already done ...'\n",
      "        return False\n",
      "   \n",
      "    # Prepare and make request.\n",
      "    message = 'ALIGNMENTS=500&DESCRIPTIONS=500&FORMAT_TYPE={format_type}&RID={rid}&CMD=Get'.format(\n",
      "        format_type=xml_or_html,\n",
      "        rid=rid,\n",
      "    )\n",
      "    request = _Request(\"http://blast.ncbi.nlm.nih.gov/Blast.cgi\",\n",
      "            message, {\"User-Agent\":\"BiopythonClient\"})\n",
      "    print 'making request ...'\n",
      "    handle = _urlopen(request)\n",
      "    time.sleep(random.randint(2, 3))\n",
      "    results = _as_string(handle.read())\n",
      "    \n",
      "    # Determine whether ready.\n",
      "    is_ready = False\n",
      "    if \"Status=\" not in results:\n",
      "        is_ready = True\n",
      "    else:\n",
      "        i = results.index(\"Status=\")\n",
      "        j = results.index(\"\\n\", i)\n",
      "        status = results[i+len(\"Status=\"):j].strip()\n",
      "        if status.upper() == \"READY\":\n",
      "            is_ready = True\n",
      "        \n",
      "    # Write if rady.\n",
      "    if is_ready:\n",
      "        print 'ready, writing results ...'\n",
      "        with open(dest, 'w') as output_fh:\n",
      "            output_fh.write(results)\n",
      "    else:\n",
      "        print 'still waiting ...'\n",
      "    \n",
      "    return True\n",
      "\n",
      "count = 1\n",
      "total = len(contig_id_to_ncbi_rid)\n",
      "for contig_id, rid in contig_id_to_ncbi_rid.iteritems():\n",
      "    print '>>>>>>>>>>>>Running %d of %d' % (count, total)\n",
      "    write_results_to_file(contig_id, rid, 'XML')\n",
      "    write_results_to_file(contig_id, rid, 'HTML')\n",
      "    count += 1"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Fix HTML Results\n",
      "\n",
      "**UPDATE:** It turns out that it's not enough to update static links to fix html files as the links to specific alignments go to a cached page (which expire after a couple days). Saving this code for posterity anyway.\n",
      "\n",
      "The HTML files have broken static links which prevent them from displaying correctly. The static links are relative rather than absolute. These can be fixed with regular expression searches."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def fix_static_html():\n",
      "    for filename in os.listdir(BLAST_XML_OUTPUT_DIR):\n",
      "        if not os.path.splitext(filename)[1] == '.html':\n",
      "            continue\n",
      "        full_path = os.path.join(BLAST_XML_OUTPUT_DIR, filename)\n",
      "        new_html_path = os.path.splitext(full_path)[0] + '.mod.html'\n",
      "        with open(new_html_path, 'w') as output_fh:\n",
      "            with open(full_path) as input_fh:\n",
      "                for line in input_fh:\n",
      "                    mod_line = line\n",
      "                    if re.search(r'link rel=\"stylesheet\"', line):\n",
      "                        mod_line = line.replace('href=\"', 'href=\"http://blast.ncbi.nlm.nih.gov/')\n",
      "                    elif re.search(r'<script.*javascript.*src=\"', line):\n",
      "                        mod_line = line.replace('src=\"', 'src=\"http://blast.ncbi.nlm.nih.gov/')          \n",
      "                    elif re.search(r'img.*src=', line):\n",
      "                        mod_line = line.replace('src=\"', 'src=\"http://blast.ncbi.nlm.nih.gov/')\n",
      "                    else:\n",
      "                        mod_line = line\n",
      "                    output_fh.write(mod_line)\n",
      "# fix_static_html()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 242
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Parse xml data"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "contig_id_to_ncbi_alignment_data_map = {}\n",
      "for filename in os.listdir(BLAST_XML_OUTPUT_DIR):\n",
      "    full_path = os.path.join(BLAST_XML_OUTPUT_DIR, filename)\n",
      "    contig_id = os.path.splitext(filename)[0]\n",
      "    assert not contig_id in contig_id_to_ncbi_alignment_data_map\n",
      "    with open(full_path) as fh:\n",
      "        parsed = NCBIXML.parse(fh)\n",
      "        x = parsed.next()\n",
      "        # We want to grab the highest alignment, unless one of them is MG1655\n",
      "        # which will make it easier to query.\n",
      "        a = x.alignments[0]\n",
      "        for other_a in x.alignments[1:30]:\n",
      "            if 'U00096.3' in other_a.hit_id:\n",
      "                a = other_a\n",
      "                break\n",
      "        h = a.hsps[0]\n",
      "        data_map = {\n",
      "            'aln_hit_id': a.hit_id,\n",
      "            'aln_hit_def': a.hit_def,\n",
      "            'aln_query_start': h.query_start,\n",
      "            'aln_query_end': h.query_end,\n",
      "            'aln_sbjct_start': h.sbjct_start,\n",
      "            'aln_sbjct_end': h.sbjct_end\n",
      "        }\n",
      "        contig_id_to_ncbi_alignment_data_map[contig_id] = data_map"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 377
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "contig_id_to_ncbi_alignment_data_map\n",
      "ncbi_alignment_data = []\n",
      "for contig_id, data in contig_id_to_ncbi_alignment_data_map.iteritems():\n",
      "    data['id'] = contig_id\n",
      "    ncbi_alignment_data.append(data)\n",
      "blast_data_df = pd.DataFrame(ncbi_alignment_data)\n",
      "blast_data_df['aln_size'] = abs(blast_data_df.aln_sbjct_end - blast_data_df.aln_sbjct_start)\n",
      "blast_data_df = sort_dataframe_columns(blast_data_df,\n",
      "        ['id', 'aln_hit_id', 'aln_hit_def', 'aln_size', 'aln_query_start', 'aln_query_end', 'aln_sbjct_start', 'aln_sbjct_end'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 378
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Combine with previous DataFrame."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "contig_data_with_blast_df = pd.merge(contig_data_df, blast_data_df, how='inner', on='id')\n",
      "contig_data_with_blast_df.sort(columns=['length', 'sample'], ascending=False).to_csv(\n",
      "        os.path.join(VELVET_DATA_DIR, 'contig_data_all_8_samples.csv'),\n",
      "        index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 379
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Scratch"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}